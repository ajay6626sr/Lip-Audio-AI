<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Lip-Audio AI Dashboard</title>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils"></script>

<style>
body {
    margin: 0;
    background: #ffffff;
    font-family: "Segoe UI", sans-serif;
}

.container {
    max-width: 1300px;
    margin: 30px auto;
    background: white;
    padding: 25px;
    border-radius: 18px;
    box-shadow: 0 12px 40px rgba(0,0,0,0.12);
}

.grid {
    display: grid;
    grid-template-columns: 1.2fr 1fr;
    gap: 25px;
}

.video-box {
    position: relative;
    background: #f1f1f1;
    border-radius: 15px;
    padding: 15px;
}

video {
    width: 100%;
    height: 430px;
    object-fit: cover;
    border-radius: 12px;
}

canvas {
    position: absolute;
    top: 15px;
    left: 15px;
    width: calc(100% - 30px);
    height: 430px;
    pointer-events: none;
}

.green { color: #28a745; font-weight: bold; }
.red { color: #dc3545; font-weight: bold; }

.energy-bar {
    height: 8px;
    background: #28a745;
    border-radius: 6px;
    width: 10%;
    transition: width 0.25s ease;
}

.controls {
    margin-top: 20px;
    display: flex;
    gap: 12px;
}

button {
    flex: 1;
    padding: 12px;
    font-size: 18px;
    border: none;
    border-radius: 10px;
    cursor: pointer;
}

.start { background: #28a745; color: white; }
.stop { background: #dc3545; color: white; }

.box {
    background: #f8f9fa;
    padding: 12px;
    border-radius: 10px;
    margin-bottom: 20px;
}
</style>
</head>

<body>
<div class="container">
<div class="grid">

<div>
    <div class="video-box">
        <video id="video" autoplay playsinline></video>
        <canvas id="canvas"></canvas>
    </div>

    <div>
        <div>Camera: <span id="camStatus" class="red">Inactive</span></div>
        <div>Lip Tracking: <span id="lipStatus" class="red">Inactive</span></div>
        <div>Live STT: <span id="sttStatus" class="red">Not Listening</span></div>
        <div class="energy-bar" id="energy"></div>
    </div>

    <div class="controls">
        <button class="start" id="startBtn">Start Live</button>
        <button class="stop" id="stopBtn">Stop & Summarize</button>
    </div>
</div>

<div>
    <div class="box">
        <b>Live Transcription</b><br/>
        <span id="liveText">Waiting...</span>
    </div>

    <div class="box">
        <b>Summary</b><br/>
        <div id="summaryText">Will appear after stopping</div>
    </div>
    <div class="box">
    <b>System Intelligence Status</b><br/>
    <div id="systemStatus">
        Initializing...
    </div>
</div>
</div>

</div>
</div>

<script>
const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");

const camStatus = document.getElementById("camStatus");
const lipStatus = document.getElementById("lipStatus");
const sttStatus = document.getElementById("sttStatus");
const energyBar = document.getElementById("energy");
const liveText = document.getElementById("liveText");
const summaryText = document.getElementById("summaryText");
const systemStatus = document.getElementById("systemStatus");

const startBtn = document.getElementById("startBtn");
const stopBtn = document.getElementById("stopBtn");

let recognition = null;
let finalTranscript = "";
let audioEnergy = 0;
let speakingScore = 0;
let activeSpeaker = "None";
let audioStream = null;
let currentWord = "";
let speakingDetected = false;
let systemStatus = document.getElementById("systemStatus");

let speakerTranscripts = {
    "Person 1": "",
    "Person 2": "",
    "Person 3": "",
    "Person 4": "",
    "Person 5": ""
};

/* ================= AI STYLE SUMMARIZER ================= */
function summarizeText(text){

    if(!text || text.length < 30)
        return "Insufficient speech data for intelligent summarization.";

    const sentences = text.match(/[^.!?]+[.!?]?/g);
    if(!sentences) return text;

    const words = text.toLowerCase().match(/\b[a-z]+\b/g) || [];

    const stopwords = new Set([
        "is","am","are","was","were","be","been","being","the","a","an",
        "and","or","but","to","of","in","on","for","with","this","that",
        "it","as","at","by","i","you","he","she","they","we","my","your",
        "our","today","hello","hi"
    ]);

    const freq = {};
    words.forEach(w=>{
        if(!stopwords.has(w))
            freq[w] = (freq[w]||0)+1;
    });

    const sortedKeywords = Object.keys(freq)
        .sort((a,b)=>freq[b]-freq[a])
        .slice(0,3);

    const keywordPhrase = sortedKeywords.join(", ");

    const nameMatch = text.match(/\b[A-Z][a-z]+\b/);
    const name = nameMatch ? nameMatch[0] : "The speaker";

    const patterns = [

        `${name} introduced the session focusing on ${keywordPhrase}.`,

        `${name} provided an overview discussing ${keywordPhrase}.`,

        `The discussion by ${name} highlighted key aspects of ${keywordPhrase}.`,

        `${name} explained important concepts related to ${keywordPhrase}.`,

        `${name} described the topic and emphasized ${keywordPhrase}.`,

        `This session covered major ideas including ${keywordPhrase}, presented by ${name}.`,

        `${name} initiated the talk and explored themes such as ${keywordPhrase}.`,

        `Key discussion points included ${keywordPhrase}, explained during the session.`,

        `${name} outlined the core subject matter involving ${keywordPhrase}.`,

        `The presentation focused on ${keywordPhrase}, introduced by ${name}.`,

        `${name} summarized insights related to ${keywordPhrase}.`,

        `Important highlights included ${keywordPhrase} as described by ${name}.`,

        `${name} presented an informative session covering ${keywordPhrase}.`,

        `The speaker elaborated on ${keywordPhrase} during the discussion.`,

        `${name} analyzed and discussed ${keywordPhrase}.`,

        `The session emphasized ${keywordPhrase}, offering structured insights.`,

        `${name} addressed critical elements such as ${keywordPhrase}.`,

        `Core concepts including ${keywordPhrase} were explained by ${name}.`,

        `${name} discussed practical and theoretical aspects of ${keywordPhrase}.`,

        `An overview of ${keywordPhrase} was provided in this session.`
    ];

    const randomIndex = Math.floor(Math.random()*patterns.length);

    return patterns[randomIndex];
}

/* ================= AUDIO ENERGY ================= */
async function initAudio() {
    if (audioStream) return;

    audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const audioCtx = new AudioContext();
    const src = audioCtx.createMediaStreamSource(audioStream);
    const analyser = audioCtx.createAnalyser();

    src.connect(analyser);
    analyser.fftSize = 256;

    const data = new Uint8Array(analyser.frequencyBinCount);

    function updateEnergy() {
        analyser.getByteFrequencyData(data);
        const sum = data.reduce((a,b)=>a+b,0);
        audioEnergy = sum/data.length/255;
        requestAnimationFrame(updateEnergy);
    }

    updateEnergy();
}

/* ================= CAMERA ================= */
async function startCamera() {
    const stream = await navigator.mediaDevices.getUserMedia({ video:true });
    video.srcObject = stream;
    camStatus.textContent="Active";
    camStatus.className="green";
}
startCamera();

/* ================= FACEMESH ================= */
const mesh = new FaceMesh({
    locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`
});

mesh.setOptions({
    maxNumFaces:5,
    refineLandmarks:true,
    minDetectionConfidence:0.5,
    minTrackingConfidence:0.5
});

mesh.onResults(res => {

    if (!video.videoWidth) return;

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    speakingDetected = false;
    activeSpeaker = "None";

    if (!res.multiFaceLandmarks || !res.multiFaceLandmarks.length) {
        lipStatus.textContent = "No Face";
        lipStatus.className = "red";
        energyBar.style.width = "10%";
        return;
    }

    lipStatus.textContent = `Tracking (${res.multiFaceLandmarks.length})`;
    lipStatus.className = "green";

    let maxMotion = 0;
    let activeSpeakerIndex = -1;

    // ---------- STEP 1: Find strongest mouth motion ----------
    res.multiFaceLandmarks.forEach((face, index) => {

        if (face[13] && face[14]) {

            const dx = face[13].x - face[14].x;
            const dy = face[13].y - face[14].y;
            const mouthOpen = Math.sqrt(dx * dx + dy * dy);

            if (mouthOpen > maxMotion) {
                maxMotion = mouthOpen;
                activeSpeakerIndex = index;
            }
        }
    });

    // ---------- STEP 2: Draw all faces ----------
    res.multiFaceLandmarks.forEach((face, index) => {

        ctx.fillStyle = "red";

        face.forEach(p => {
            ctx.beginPath();
            ctx.arc(p.x * canvas.width, p.y * canvas.height, 1.2, 0, 2 * Math.PI);
            ctx.fill();
        });

        // Only highlight strongest speaker
        if (index === activeSpeakerIndex && maxMotion > 0.02 && audioEnergy > 0.015) {

            speakingDetected = true;
            activeSpeaker = "Person " + (index + 1);

            const mouthPoint = face[13];

            // Green lip highlight
            ctx.strokeStyle = "#00ff00";
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.arc(mouthPoint.x * canvas.width,
                    mouthPoint.y * canvas.height,
                    20, 0, 2 * Math.PI);
            ctx.stroke();

            // Word bubble text
            ctx.fillStyle = "yellow";
            ctx.font = "bold 18px Segoe UI";
            ctx.fillText(
                activeSpeaker + " Speaking",
                mouthPoint.x * canvas.width - 60,
                mouthPoint.y * canvas.height - 30
            );

            speakingScore = 0.9 * speakingScore + 0.1 * maxMotion;
        }
    });

    // ---------- STEP 3: Update system status ----------
    systemStatus.innerHTML =
        "✓ Lip Motion Tracking Active<br>" +
        (audioEnergy > 0.015 ? "✓ Audio Energy Validated<br>" : "✗ Audio Energy Low<br>") +
        (speakingDetected ? "✓ Multimodal Sync Active<br>" : "✗ Waiting for Speech<br>") +
        "✓ AI Extractive Summarization Ready";

    energyBar.style.width = speakingScore > 0.018 ? "100%" : "10%";
});
new Camera(video,{
    onFrame:async()=>await mesh.send({image:video})
}).start();

/* ================= SPEECH RECOGNITION ================= */
const SpeechRecognition=window.SpeechRecognition||window.webkitSpeechRecognition;

startBtn.onclick=async()=>{

    if(!SpeechRecognition){
        liveText.textContent="SpeechRecognition not supported";
        return;
    }

    await initAudio();

    recognition=new SpeechRecognition();
    recognition.continuous=true;
    recognition.interimResults=true;
    recognition.lang="en-US";

    recognition.onresult=e=>{
        let interim="";

        for(let i=e.resultIndex;i<e.results.length;i++){
            const txt=e.results[i][0].transcript;
            const words=txt.trim().split(" ");
            currentWord = words.slice(-2).join(" ");
            if(e.results[i].isFinal){
                speakerTranscripts[activeSpeaker]+=txt+" ";
                finalTranscript+=txt+" ";
            } else {
                interim+=txt;
            }
        }

        let display="";
        Object.keys(speakerTranscripts).forEach(s=>{
            if(speakerTranscripts[s])
                display+=s+": "+speakerTranscripts[s]+"\n\n";
        });

        liveText.textContent=display||"Listening...";
    };

    recognition.start();
    sttStatus.textContent="Listening...";
    sttStatus.className="green";
};

stopBtn.onclick=()=>{
    sttStatus.textContent="Not Listening";
    sttStatus.className="red";

    if(recognition){
        recognition.stop();
        recognition=null;
    }

    summaryText.textContent=summarizeText(finalTranscript);
};
</script>

</body>

</html>
