<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Lip-Audio AI Dashboard</title>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils"></script>

<style>
body {
    margin: 0;
    background: #ffffff;
    font-family: "Segoe UI", sans-serif;
}

.container {
    max-width: 1300px;
    margin: 30px auto;
    background: white;
    padding: 25px;
    border-radius: 18px;
    box-shadow: 0 12px 40px rgba(0,0,0,0.12);
}

.grid {
    display: grid;
    grid-template-columns: 1.2fr 1fr;
    gap: 25px;
}

.video-box {
    position: relative;
    background: #f1f1f1;
    border-radius: 15px;
    padding: 15px;
}

video {
    width: 100%;
    height: 430px;
    object-fit: cover;
    border-radius: 12px;
}

canvas {
    position: absolute;
    top: 15px;
    left: 15px;
    width: calc(100% - 30px);
    height: 430px;
    pointer-events: none;
}

.green { color: #28a745; font-weight: bold; }
.red { color: #dc3545; font-weight: bold; }

.energy-bar {
    height: 8px;
    background: #28a745;
    border-radius: 6px;
    width: 10%;
    transition: width 0.25s ease;
}

.controls {
    margin-top: 20px;
    display: flex;
    gap: 12px;
}

button {
    flex: 1;
    padding: 12px;
    font-size: 18px;
    border: none;
    border-radius: 10px;
    cursor: pointer;
}

.start { background: #28a745; color: white; }
.stop { background: #dc3545; color: white; }

.box {
    background: #f8f9fa;
    padding: 12px;
    border-radius: 10px;
    margin-bottom: 20px;
}
</style>
</head>

<body>
<div class="container">
<div class="grid">

<div>
    <div class="video-box">
        <video id="video" autoplay playsinline></video>
        <canvas id="canvas"></canvas>
    </div>

    <div>
        <div>Camera: <span id="camStatus" class="red">Inactive</span></div>
        <div>Lip Tracking: <span id="lipStatus" class="red">Inactive</span></div>
        <div>Live STT: <span id="sttStatus" class="red">Not Listening</span></div>
        <div class="energy-bar" id="energy"></div>
    </div>

    <div class="controls">
        <button class="start" id="startBtn">Start Live</button>
        <button class="stop" id="stopBtn">Stop & Summarize</button>
    </div>
</div>

<div>
    <div class="box">
        <b>Live Transcription</b><br/>
        <span id="liveText">Waiting...</span>
    </div>

    <div class="box">
        <b>Summary</b><br/>
        <div id="summaryText">Will appear after stopping</div>
    </div>
    <div class="box">
    <b>System Intelligence Status</b><br/>
    <div id="systemStatus">
        Initializing...
    </div>
</div>
</div>

</div>
</div>

<script>
const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");

const camStatus = document.getElementById("camStatus");
const lipStatus = document.getElementById("lipStatus");
const sttStatus = document.getElementById("sttStatus");
const energyBar = document.getElementById("energy");
const liveText = document.getElementById("liveText");
const summaryText = document.getElementById("summaryText");
const systemStatus = document.getElementById("systemStatus");

const startBtn = document.getElementById("startBtn");
const stopBtn = document.getElementById("stopBtn");

let recognition = null;
let finalTranscript = "";
let audioEnergy = 0;
let ambientNoise = 0;
let audioStream = null;
let currentWord = "";
let confidenceScore = 0;

let lastSpeakerIndex = -1;
let speakerHoldFrames = 0;

let speakerTranscripts = {
    "Person 1": "",
    "Person 2": "",
    "Person 3": "",
    "Person 4": "",
    "Person 5": ""
};

/* ================= SUMMARIZER ================= */
function summarizeText(text){
    if(!text || text.length < 40)
        return "Speech data insufficient for intelligent summarization.";

    const words = text.toLowerCase().match(/\b[a-z]+\b/g) || [];
    const stopwords = new Set(["is","am","are","was","were","be","been","being","the","a","an","and","or","but","to","of","in","on","for","with","this","that","it","as","at","by","i","you","he","she","they","we","my","your","our"]);

    const freq = {};
    words.forEach(w=>{
        if(!stopwords.has(w))
            freq[w]=(freq[w]||0)+1;
    });

    const topKeywords = Object.keys(freq)
        .sort((a,b)=>freq[b]-freq[a])
        .slice(0,4);

    const topic = topKeywords.join(", ");

    const templates = [
        `The session discussed key topics including ${topic}.`,
        `Important concepts such as ${topic} were explained.`,
        `The discussion focused on ${topic}.`,
        `Major highlights revolved around ${topic}.`
    ];

    return templates[Math.floor(Math.random()*templates.length)];
}

/* ================= AUDIO ================= */
async function initAudio(){
    if(audioStream) return;

    audioStream = await navigator.mediaDevices.getUserMedia({audio:true});
    const audioCtx = new AudioContext();
    const src = audioCtx.createMediaStreamSource(audioStream);
    const analyser = audioCtx.createAnalyser();

    src.connect(analyser);
    analyser.fftSize = 256;

    const data = new Uint8Array(analyser.frequencyBinCount);

    function updateEnergy(){
        analyser.getByteFrequencyData(data);
        const raw = data.reduce((a,b)=>a+b,0)/data.length/255;

        audioEnergy = 0.8*audioEnergy + 0.2*raw;
        ambientNoise = 0.95*ambientNoise + 0.05*audioEnergy;

        requestAnimationFrame(updateEnergy);
    }
    updateEnergy();
}

/* ================= CAMERA ================= */
async function startCamera(){
    const stream = await navigator.mediaDevices.getUserMedia({video:true});
    video.srcObject = stream;
    camStatus.textContent="Active";
    camStatus.className="green";
}
startCamera();

/* ================= FACEMESH ================= */
const mesh = new FaceMesh({
    locateFile:f=>`https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`
});

mesh.setOptions({
    maxNumFaces:5,
    refineLandmarks:true,
    minDetectionConfidence:0.5,
    minTrackingConfidence:0.5
});

mesh.onResults(res=>{
    if(!video.videoWidth) return;

    canvas.width=video.videoWidth;
    canvas.height=video.videoHeight;
    ctx.clearRect(0,0,canvas.width,canvas.height);

    if(!res.multiFaceLandmarks || !res.multiFaceLandmarks.length){
        lipStatus.textContent="No Face";
        lipStatus.className="red";
        return;
    }

    lipStatus.textContent=`Tracking (${res.multiFaceLandmarks.length})`;
    lipStatus.className="green";

    // Leftmost face = Person 1
    const faceOrder = res.multiFaceLandmarks
        .map((face,index)=>({index,x:face[1].x}))
        .sort((a,b)=>a.x-b.x);

    let maxMotion=0;
    let detectedIndex=-1;

    res.multiFaceLandmarks.forEach((face,index)=>{
        const dx=face[13].x-face[14].x;
        const dy=face[13].y-face[14].y;
        const mouthOpen=Math.sqrt(dx*dx+dy*dy);

        if(mouthOpen>maxMotion){
            maxMotion=mouthOpen;
            detectedIndex=index;
        }
    });

    // Speaker smoothing (fixed)
    if(lastSpeakerIndex === -1){
        lastSpeakerIndex = detectedIndex;
    }

    if(detectedIndex === lastSpeakerIndex){
        speakerHoldFrames++;
    } else {
        speakerHoldFrames = 0;
    }

    if(speakerHoldFrames > 3){
        lastSpeakerIndex = detectedIndex;
    }

    const activeIndex = lastSpeakerIndex;

    res.multiFaceLandmarks.forEach((face,index)=>{

        ctx.fillStyle="red";
        face.forEach(p=>{
            ctx.beginPath();
            ctx.arc(p.x*canvas.width,p.y*canvas.height,1.2,0,2*Math.PI);
            ctx.fill();
        });

        if(index===activeIndex && maxMotion>0.02 && audioEnergy>(ambientNoise+0.01)){

            const stableID = faceOrder.findIndex(f=>f.index===index);
            const speakerName="Person "+(stableID+1);

            // Green lip contour
            const outerLips=[61,146,91,181,84,17,314,405,321,375,291];

            ctx.strokeStyle="#00ff00";
            ctx.lineWidth=2;
            ctx.beginPath();
            outerLips.forEach((id,i)=>{
                const p=face[id];
                const x=p.x*canvas.width;
                const y=p.y*canvas.height;
                if(i===0) ctx.moveTo(x,y);
                else ctx.lineTo(x,y);
            });
            ctx.closePath();
            ctx.stroke();

            // Word bubble
            ctx.fillStyle="yellow";
            ctx.font="bold 18px Segoe UI";
            ctx.fillText(
                speakerName+": "+currentWord,
                face[13].x*canvas.width-60,
                face[13].y*canvas.height-30
            );
        }
    });

    // Confidence blending (fixed scaling)
    const lipConf=Math.min(1,maxMotion*20);
    const audioConf=Math.min(1,audioEnergy*3);
    confidenceScore=Math.round((lipConf*0.6+audioConf*0.4)*100);

    energyBar.style.width=confidenceScore+"%";

    systemStatus.innerHTML=
        "✓ Lip Motion Tracking Active<br>"+
        (audioEnergy>(ambientNoise+0.01)?"✓ Audio Energy Validated<br>":"✗ Audio Energy Low<br>")+
        "✓ AI Extractive Summarization Ready<br>"+
        "✓ Multimodal Confidence: "+confidenceScore+"%";
});

new Camera(video,{
    onFrame:async()=>await mesh.send({image:video})
}).start();

/* ================= SPEECH RECOGNITION ================= */
const SpeechRecognition=window.SpeechRecognition||window.webkitSpeechRecognition;

startBtn.onclick=async()=>{
    if(!SpeechRecognition){
        liveText.textContent="SpeechRecognition not supported";
        return;
    }

    await initAudio();

    recognition=new SpeechRecognition();
    recognition.continuous=true;
    recognition.interimResults=true;
    recognition.lang="en-US";

    recognition.onresult=e=>{
        for(let i=e.resultIndex;i<e.results.length;i++){
            const txt=e.results[i][0].transcript;
            const words=txt.trim().split(" ");
            currentWord=words.slice(-1).join(" ");

            if(e.results[i].isFinal){
                finalTranscript+=txt+" ";
            }
        }

        liveText.textContent=finalTranscript || "Listening...";
    };

    recognition.start();
    sttStatus.textContent="Listening...";
    sttStatus.className="green";
};

stopBtn.onclick=()=>{
    sttStatus.textContent="Not Listening";
    sttStatus.className="red";

    if(recognition){
        recognition.stop();
        recognition=null;
    }

    summaryText.textContent=summarizeText(finalTranscript);
};
</script>
</body>

</html>



